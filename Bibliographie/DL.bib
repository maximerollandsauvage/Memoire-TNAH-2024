@book{andlerIntelligenceArtificielleIntelligence2023,
  title = {{Intelligence artificielle, intelligence humaine: la double {\'e}nigme}},
  shorttitle = {{Intelligence artificielle, intelligence humaine}},
  author = {Andler, Daniel},
  year = {2023},
  series = {{NRF essais}},
  publisher = {{Gallimard}},
  address = {{Paris}}, 
  isbn = {978-2-07-279288-5},
  langid = {fre},
  lccn = {006.301},
  keywords = {DL}
}

@book{BigDataMachine2019,
  title = {{Big data et machine learning: les concepts et les outils de la data science}},
  shorttitle = {{Big data et machine learning}},
  author = {Batty, Marc and Raffaelli, Jean-Luc},
  year = {2019},
  series = {{InfoPro}},
  edition = {3 {\'e}d},
  publisher = {Dunod},
  address = {Malakoff},
  isbn = {978-2-10-079037-1},
  langid = {fre},
  lccn = {658.403 8011},
  keywords = {DL}
}

@book{biernatDataScienceFondamentaux2015,
  title = {{Data science: fondamentaux et {\'e}tudes de cas machine learning avec Python et R}},
  shorttitle = {{Data science}},
  author = {Biernat, {\'E}ric and Lutz, Michel},
  year = {2015},
  publisher = {Eyrolles},
  address = {Paris},
  isbn = {978-2-212-14243-3},
  langid = {fre},
  lccn = {006.31},
  keywords = {DL}
}

@article{coquenetDANSegmentationFreeDocument2023,
  title = {{{DAN}}: {{A Segmentation-Free Document Attention Network}} for {{Handwritten Document Recognition}}},
  shorttitle = {{{DAN}}},
  author = {Coquenet, Denis and Chatelain, Cl{\'e}ment and Paquet, Thierry},
  year = {2023},
  month = jul,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {45},
  number = {7},
  pages = {8227--8243},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2023.3235826},
  urldate = {2024-08-28},
  abstract = {Unconstrained handwritten text recognition is a challenging computer vision task. It is traditionally handled by a two-step approach, combining line segmentation followed by text line recognition. For the first time, we propose an end-to-end segmentation-free architecture for the task of handwritten document recognition: the Document Attention Network. In addition to text recognition, the model is trained to label text parts using begin and end tags in an XML-like fashion. This model is made up of an FCN encoder for feature extraction and a stack of transformer decoder layers for a recurrent token-by-token prediction process. It takes whole text documents as input and sequentially outputs characters, as well as logical layout tokens. Contrary to the existing segmentation-based approaches, the model is trained without using any segmentation label. We achieve competitive results on the READ 2016 dataset at page level, as well as double-page level with a CER of 3.43\% and 3.70\%, respectively. We also provide results for the RIMES 2009 dataset at page level, reaching 4.54\% of CER. We provide all source code and pre-trained model weights at https://github.com/FactoDeepLearning/DAN.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {C:\Users\DELL\Desktop\MEMOIRE\Coq22a.pdf},
  keywords = {DL}
}

@book{ganasciaIntelligenceArtificielleVers2017,
  title = {Intelligence Artificielle: Vers Une Domination Programm{\'e}e?},
  shorttitle = {Intelligence Artificielle},
  author = {Ganascia, Jean-Gabriel},
  year = {2017},
  series = {Id{\'e}es Re{\c c}ues},
  edition = {2e {\'e}dition revue et augment{\'e}e},
  publisher = {{Le Cavalier bleu {\'e}ditions}},
  address = {{Paris}},
  isbn = {979-10-318-0213-8},
  lccn = {Q335 .G3563 2017},
  keywords = {DL}
}

@misc{weiEmergentAbilitiesLarge2022a,
  title = {Emergent {{Abilities}} of {{Large Language Models}}},
  author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
  year = {2022},
  month = oct,
  number = {arXiv:2206.07682},
  eprint = {2206.07682},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2024-08-28},
  abstract = {Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence raises the question of whether additional scaling could potentially further expand the range of capabilities of language models.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {E:\Memoire\Emergent Abilities of Large Language Models.pdf},
  keywords = {DL}
}
